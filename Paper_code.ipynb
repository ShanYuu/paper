{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SP500公司名稱前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanco import prepare_terms, basename # 處理公司symbol\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.utils import quote\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 根據SP500的 Company name搜集\n",
    "read = os.path.join(os.getcwd(),'SP500_merge.xlsx')\n",
    "symbol_data = pd.read_excel(read, index_col = 0 )\n",
    "symbol = symbol_data['Company name'].to_list()\n",
    "\n",
    "# 去除常見贅字\n",
    "corp_name = []\n",
    "for i, element in enumerate(list(symbol)):\n",
    "    ele_low = element.lower()\n",
    "    ele_low = ele_low.replace(\" company\", '').replace(' corp', '').replace(' inc', '')\\\n",
    "    .replace(\" plc\", '').replace(' class a', '').replace(' class c', '')\\\n",
    "    .replace(\" class b\", '').replace(\" ltd.\", \"\").replace(\".com\", \"\")\\\n",
    "    .replace(\" int'l\", \"\").replace(\" corporation\", \"\").replace(\" group\", \"\")\\\n",
    "    .replace(\" svc.gp.\", \"\").replace(\" incorporated\", \"\")\n",
    "    corp_name.append(ele_low)\n",
    "\n",
    "# 部分company 例外處理\n",
    "corp_name[324] = 'News Class A'\n",
    "corp_name[325] = 'News Class B'\n",
    "corp_name[198] = 'Gap Inc.'\n",
    "# print(corp_name[2], symbol_data.iloc[2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除意義模糊的公司代碼\n",
    "symbol_blacklist = ['MMM', 'ARE', 'AES', 'ALL', 'ABC', 'BLL', 'CAH', 'CAT',\\\n",
    "                   'CSX', 'CME', 'COST', 'CVS', 'EBAY', 'FMC', 'GIS', 'HES',\\\n",
    "                   'ICE', 'KEY', 'LIN', 'LEN', 'LOW', 'MET', 'ROST', 'LUV',\\\n",
    "                   'TEL', 'TXT', 'COO', 'MOS', 'DIS', 'USB', 'BIO','HAS',\n",
    "                   'NOW','WELL','POOL','FAST','SEE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.蒐集CNN新聞資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(data_list):\n",
    "    page = 1\n",
    "    while True:\n",
    "        time.sleep(random.randint(2,3))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        corp_arti = soup.find_all(class_= \"cnn-search__result--article\")\n",
    "\n",
    "        for arti in corp_arti:\n",
    "            data.append({\n",
    "                'headline': arti.find(class_ = 'cnn-search__result-headline').text.replace('\\n', ''),\n",
    "                'date'    : arti.find(class_ = 'cnn-search__result-publish-date').text.replace('\\n', ''),\n",
    "                'link'    : arti.find('a')['href'].replace('//', ''),\n",
    "                'text'    : arti.find(class_ = 'cnn-search__result-body').text.replace('\\n', '').replace(\"  \", ''),\n",
    "            })\n",
    "        print('page: ', page, \" total_data: \", len(data))    \n",
    "        #換頁(Next)\n",
    "        try:\n",
    "            #如果Next存在\n",
    "            e = driver.find_element_by_css_selector('body > div.pg-search.pg-wrapper > div.pg-no-rail.pg-wrapper > div > div.l-container > div.cnn-search__right > div > div.cnn-search__results-pagi > div > div.pagination-arrow.pagination-arrow-right.cnnSearchPageLink.text-active')\n",
    "            e.click()\n",
    "\n",
    "            if page == 1000:#頁數超過1000頁，無法抓取多出的data\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "        page += 1\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 網站的新聞類別\n",
    "category = ['us,politics,world,opinion,health', 'entertainment', 'sport', 'travel', 'style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options,executable_path='/usr/bin/geckodriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(4,len(category)) :\n",
    "    category_name = category[i]\n",
    "    print(\"********\" , category_name, \"***********\")\n",
    "    num  = 0\n",
    "    for k  in range(0,len(corp_name)):\n",
    "        if num == 5: # 避免網頁當掉\n",
    "            driver.quit()\n",
    "            options.headless = True\n",
    "            driver = webdriver.Firefox(options=options,executable_path='/usr/bin/geckodriver')\n",
    "\n",
    "        # 根據Company Name蒐集資料\n",
    "        print(\"----\",k, corp_name[k],\"----\")\n",
    "        name_qoute = quote(corp_name[k], safe='')\n",
    "        url = 'https://edition.cnn.com/search?q='+ name_qoute +'&size=10&type=article&'\\\n",
    "        +'category=' + category_name\n",
    "        driver.get(url)\n",
    "\n",
    "        data = []\n",
    "\n",
    "        data = search(data) #def\n",
    "\n",
    "        # 根據Symbol搜集資料\n",
    "        if (symbol_data.iloc[k,0] not in symbol_blacklist) and (len(symbol_data.iloc[k,0]) > 2):\n",
    "            name_qoute = quote(symbol_data.iloc[k,0], safe='')\n",
    "            url = 'https://edition.cnn.com/search?q='+ name_qoute +'&size=10&type=article&'\\\n",
    "            +'category=' + category_name\n",
    "            driver.get(url)\n",
    "            search(data) #def\n",
    "\n",
    "        ## 判別是否有資料，有的話存成csv，沒有的話另外紀錄名稱為txt，以便核對\n",
    "        if len(data) != 0: \n",
    "            links_data = pd.DataFrame(data)\n",
    "            links_data = links_data.drop_duplicates(subset=['link'])\n",
    "            print(\"data_len\", len(data))\n",
    "            out = os.path.join(category_name , symbol_data.iloc[k,0]+'.csv')\n",
    "            links_data.to_csv(out, index = False)\n",
    "        else:\n",
    "            print(symbol_data.iloc[k,0], 'No data')\n",
    "            with open(os.path.join(category_name+'_NoData_file.txt'), 'a') as f:\n",
    "                f.write( symbol_data.iloc[k,0] + \"\\n\" )\n",
    "        num += 1\n",
    "#     break\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.量化新聞資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pysentiment2 as ps\n",
    "import string\n",
    "import xlrd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "處理token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "#忽略隱藏檔\n",
    "files = [i for i in os.listdir(folder_path) if not i.startswith('.')]\n",
    "\n",
    "for i in range(len(files)):\n",
    "    data_read = os.path.join(folder_path, files[i] )\n",
    "    comment_data = pd.read_csv(data_read)\n",
    "    print(i, data_read)\n",
    "    \n",
    "    comment_data['headline_token'] = None\n",
    "    comment_data['text_token']  = None\n",
    "    \n",
    "    # 計算共有幾則評論\n",
    "    review_num = 0 \n",
    "\n",
    "    # title\n",
    "    for j in range(0, len(comment_data)):\n",
    "        content = str(comment_data['headline'][j]).lower()\n",
    "        # 移除標點符號\n",
    "        text = ''.join(ch for ch in content if ch not in exclude)\n",
    "        # tokeing \n",
    "        tokens = word_tokenize(text)\n",
    "        # 移除 stopwords\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # 將 token 連成 str, 使用 , 連結\n",
    "        comment_data.iloc[j, -2] = ','.join(x for x in tokens)\n",
    "\n",
    "    # content\n",
    "    for k in range(0, len(comment_data)):\n",
    "        content = str(comment_data['text'][k]).lower()\n",
    "        # 移除標點符號\n",
    "        text = ''.join(ch for ch in content if ch not in exclude)\n",
    "        # tokeing \n",
    "        tokens = word_tokenize(text)\n",
    "        # 移除 stopwords\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # 將 token 連成 str, 使用 , 連結\n",
    "        comment_data.iloc[k, -1] = ','.join(x for x in tokens)\n",
    "        \n",
    "        review_num += 1\n",
    "    csv = files[i].replace('.csv', '_token.csv')\n",
    "    output = os.path.join(os.getcwd(),'sanfish','Business_token', csv)\n",
    "    comment_data.to_csv(output, index = True)\n",
    "    print(\"reviews: \", review_num)\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算情緒詞彙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiv4 = ps.HIV4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [i for i in os.listdir(folder_path) if not i.startswith('.')]\n",
    "for i in range(len(files)):\n",
    "    data_read = os.path.join(folder_path, files[i])\n",
    "    token_data = pd.read_csv(data_read, index_col=0)\n",
    "    print('**', i , files[i], 'len:', len(token_data), '**')\n",
    "    \n",
    "    token_data[\"headline_pos\"]      = None #6\n",
    "    token_data[\"headline_neg\"]      = None #7\n",
    "    token_data[\"headline_netural\"]  = None #8\n",
    "    token_data[\"headline_total\"]    = None #9\n",
    "    \n",
    "    token_data[\"text_pos\"]      = None #10\n",
    "    token_data[\"text_neg\"]      = None #11\n",
    "    token_data[\"text_netural\"]  = None #12\n",
    "    token_data[\"text_total\"]    = None #13\n",
    "    \n",
    "    for j in range(0, token_data.shape[0]):\n",
    "        if(j%100==0):\n",
    "            print(j)\n",
    "    # title\n",
    "        title_pos = 0\n",
    "        title_neg = 0\n",
    "        title_netural = 0\n",
    "        title_total   = 0\n",
    "        if type(token_data[\"headline_token\"][j]) != float:\n",
    "            for title_word in token_data[\"headline_token\"][j].split(\",\"):\n",
    "                tokens = hiv4.tokenize(title_word)\n",
    "                score = hiv4.get_score(tokens)\n",
    "                \n",
    "                # 分類詞性\n",
    "                if(score[\"Polarity\"] > 0):\n",
    "                    title_pos += 1\n",
    "                elif (score[\"Polarity\"] < 0) :\n",
    "                    title_neg += 1\n",
    "                else:\n",
    "                    title_netural += 1\n",
    "                token_data.iloc[j, 6] = title_pos\n",
    "                token_data.iloc[j, 7] = title_neg\n",
    "                token_data.iloc[j, 8] = title_netural\n",
    "                token_data.iloc[j, 9] = len(token_data[\"headline_token\"][j].split(\",\"))\n",
    "        # text\n",
    "        text_pos = 0\n",
    "        text_neg = 0\n",
    "        text_netural = 0\n",
    "        text_total   = 0\n",
    "        if type(token_data[\"text_token\"][j]) != float:\n",
    "            for word in token_data[\"text_token\"][j].split(\",\"):\n",
    "                tokens = hiv4.tokenize(word)\n",
    "                score = hiv4.get_score(tokens)\n",
    "                if(score[\"Polarity\"] > 0):\n",
    "                    text_pos += 1\n",
    "                elif (score[\"Polarity\"] < 0) :\n",
    "                    text_neg += 1\n",
    "                else:\n",
    "                    text_netural += 1\n",
    "                token_data.iloc[j, -4] = text_pos\n",
    "                token_data.iloc[j, -3] = text_neg\n",
    "                token_data.iloc[j, -2] = text_netural\n",
    "                token_data.iloc[j, -1] = len(token_data[\"text_token\"][j].split(\",\"))\n",
    "    csv = files[i].replace('_token.csv', '_analyzed.csv')\n",
    "    output = os.path.join(os.getcwd(),'Business2','Business_analyzed',csv)\n",
    "    token_data.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = os.path.join(os.getcwd(),'SP500_merge.xlsx')\n",
    "symbol_data = pd.read_excel(read, index_col = 0 )\n",
    "symbol = symbol_data['Company name'].to_list()\n",
    "\n",
    "\n",
    "corp_name = []\n",
    "for i, element in enumerate(list(symbol)):\n",
    "    ele_low = element.lower()\n",
    "    ele_low = ele_low.replace(\" company\", '').replace(' corp', '').replace(' inc', '')\\\n",
    "    .replace(\" plc\", '').replace(' class a', '').replace(' class c', '')\\\n",
    "    .replace(\" class b\", '').replace(\" ltd.\", \"\").replace(\".com\", \"\")\\\n",
    "    .replace(\" int'l\", \"\").replace(\" corporation\", \"\").replace(\" group\", \"\")\\\n",
    "    .replace(\" svc.gp.\", \"\").replace(\" incorporated\", \"\")\n",
    "    corp_name.append(ele_low)\n",
    "\n",
    "# 部分company\n",
    "corp_name[324] = 'News Class A'\n",
    "corp_name[325] = 'News Class B'\n",
    "corp_name[198] = 'Gap Inc.'\n",
    "print(corp_name[3], symbol_data.iloc[3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_blacklist = ['MMM', 'ARE', 'AES', 'ALL', 'ABC', 'BLL', 'CAH', 'CAT',\\\n",
    "                   'CSX', 'CME', 'COST', 'CVS', 'EBAY', 'FMC', 'GIS', 'HES',\\\n",
    "                   'ICE', 'KEY', 'LIN', 'LEN', 'LOW', 'MET', 'ROST', 'LUV',\\\n",
    "                   'TEL', 'TXT', 'COO', 'MOS', 'DIS', 'USB', 'BIO','HAS',\n",
    "                   'NOW','WELL','POOL','FAST','SEE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(),'travel1','travel_analyzed')\n",
    "files = [i for i in os.listdir(folder_path) if not i.startswith('.')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.過濾新聞內文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.getcwd(),'travel1','travel_analyzed')\n",
    "files = [i for i in os.listdir(folder_path) if not i.startswith('.')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(files)):\n",
    "    f = files[j]\n",
    "    sym = f.replace('_analyzed.csv','')\n",
    "    i = symbol_data[symbol_data[\"Symbol\"]==sym].index[0]\n",
    "    print(j,'index', i ,sym)\n",
    "    data_read = os.path.join(folder_path, f)\n",
    "    token_data = pd.read_csv(data_read)\n",
    "    token_data = token_data.dropna(subset = ['text']) # 確保text沒有空值\n",
    "    \n",
    "    lower = token_data['text'].str.lower()\n",
    "    c1 = lower.str.contains(corp_name[i]) #確認公司名稱是否在本文裡    \n",
    "    \n",
    "    if len(sym) < 3 or sym in symbol_blacklist: #如果len(symbol)<3 or 在沒有搜尋的名單中\n",
    "        token_data2 = token_data[ c1 ]\n",
    "        print('  ***', 'c1', Counter(lower.str.contains(corp_name[i])), 'len2:', len(token_data2))\n",
    "\n",
    "    else:\n",
    "        c2 = token_data['text'].str.contains(symbol_data.iloc[i,0])\n",
    "        c = [a or b for a, b in zip(c1, c2)]\n",
    "        token_data2 = token_data[c]\n",
    "        print('  ***','len1:', len(token_data),'len2:', len(token_data2))\n",
    "    \n",
    "    csv = f.replace('_token.csv', '_analyzed.csv')\n",
    "    output = os.path.join(os.getcwd(),'travel2','travel_analyzed', csv)\n",
    "    token_data2.to_csv(output, index= False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in category_list:\n",
    "    news_path = os.path.join(os.getcwd(), category+'2', category+'_analyzed')\n",
    "    print(news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將每家公司不同類別的新聞合併成一個檔案\n",
    "\n",
    "symbol_dic = defaultdict()\n",
    "for category in category_list:\n",
    "    print(category)\n",
    "    \n",
    "    news_path = os.path.join(os.getcwd(), category+'2', category+'_analyzed')\n",
    "#     print(news_path)\n",
    "    news_files = [i for i in os.listdir(news_path) if not i.startswith('.')]\n",
    "    \n",
    "    for j in range(len(symbol_list)):\n",
    "        symbol = symbol_list[j]# 公司代碼\n",
    "#         print(symbol)\n",
    "        try:\n",
    "            if symbol+'_analyzed.csv' in news_files:\n",
    "                \n",
    "                read_path = os.path.join(news_path, symbol+'_analyzed.csv') # 讀取data的路徑\n",
    "                news_data = pd.read_csv(read_path)\n",
    "\n",
    "                if symbol in symbol_dic.keys():\n",
    "                    symbol_dic[symbol] = pd.concat([symbol_dic[symbol], news_data] ) \n",
    "                else:\n",
    "                    symbol_dic[symbol] = news_data\n",
    "        \n",
    "        except NotADirectoryError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根據key 將每間公司存成獨立檔案\n",
    "for key, value in symbol_dic.items():\n",
    "    print(key)\n",
    "    dateFormatter = \"%b-%d-%Y\"\n",
    "    value['date'] = value['date'].apply(lambda x: x.replace(' ', '-').replace(',',''))\n",
    "    value['date'] = value['date'].apply(lambda x: datetime.strptime(x, dateFormatter).date())\n",
    "    csv = key + '_analyzed.csv'\n",
    "    out_path = os.path.join(os.getcwd(),'CNN_NoCategory2','CNN_NoCategory_analyzed', csv)\n",
    "    value.to_csv( out_path, index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.合併日期＆計算情緒指數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(files)):\n",
    "    data_read = os.path.join(folder_path, files[i])\n",
    "    merged_data = pd.read_csv(data_read)\n",
    "    \n",
    "    merged_data[\"amount\"] = 1\n",
    "    aggregation_functions = {\n",
    "            'headline_pos': 'sum', 'headline_neg': \"sum\",'headline_netural': 'sum', 'headline_total': 'sum',\\\n",
    "            'text_pos': 'sum', 'text_neg': \"sum\",'text_netural': 'sum', 'text_total': 'sum',\\\n",
    "            'amount': \"sum\" }\n",
    "    merged_data = merged_data.groupby(merged_data['date']).aggregate(aggregation_functions)\n",
    "    merged_data = merged_data.reset_index()\n",
    "    \n",
    "    merged_data['headline_Pos_ratio']     = None #10\n",
    "    merged_data['headline_Neg_ratio']     = None #11\n",
    "    merged_data['headline_Netural_ratio'] = None #12\n",
    "    merged_data['headline_Polarity']      = None #13\n",
    "    merged_data['headline_Subjectivity']  = None #14\n",
    "    \n",
    "    merged_data['text_Pos_ratio']         = None #15\n",
    "    merged_data['text_Neg_ratio']         = None #16\n",
    "    merged_data['text_Netural_ratio']     = None #17\n",
    "    merged_data['text_Polarity']          = None #18\n",
    "    merged_data['text_Subjectivity']      = None #19\n",
    "    \n",
    "    for w in range(0, merged_data.shape[0]):\n",
    "        if(merged_data[\"headline_pos\"].iloc[w] + merged_data[\"headline_neg\"].iloc[w]!=0):#分母不可為0\n",
    "            merged_data.iloc[w, 10] = merged_data[\"headline_pos\"].iloc[w]\\\n",
    "            /(merged_data[\"headline_pos\"].iloc[w]+merged_data[\"headline_neg\"].iloc[w])#head_Pos_ratio\n",
    "\n",
    "            merged_data.iloc[w, 11] = merged_data[\"headline_neg\"].iloc[w]\\\n",
    "            /(merged_data[\"headline_pos\"].iloc[w]+merged_data[\"headline_neg\"].iloc[w])#head_Neg_ratio\n",
    "\n",
    "            merged_data.iloc[w, 13] = (merged_data[\"headline_pos\"].iloc[w]-merged_data[\"headline_neg\"].iloc[w])\\\n",
    "            /(merged_data[\"headline_pos\"].iloc[w]+merged_data[\"headline_neg\"].iloc[w])#head_Polarity\n",
    "        else:\n",
    "            merged_data.iloc[w, 10] = 0 #head_Pos_ratio\n",
    "            merged_data.iloc[w, 11] = 0 #head_Neg_ratio        \n",
    "            merged_data.iloc[w, 13] = 0 #head_Polarity\n",
    "\n",
    "        if(merged_data[\"headline_total\"].iloc[w]!=0):#分母不可為0\n",
    "            merged_data.iloc[w, 12] = merged_data[\"headline_netural\"].iloc[w]/merged_data[\"headline_total\"].iloc[w]#head_Netural_ratio\n",
    "\n",
    "            merged_data.iloc[w, 14] = (merged_data[\"headline_pos\"].iloc[w]+merged_data[\"headline_neg\"].iloc[w])/ \\\n",
    "            (merged_data[\"headline_total\"].iloc[w])#head_Subjectivity\n",
    "        else:\n",
    "            merged_data.iloc[w, 12] = 0 #head_Netural_ratio\n",
    "            merged_data.iloc[w, 14] = 0 #head_Subjectivity\n",
    "\n",
    "\n",
    "        if(merged_data[\"text_pos\"].iloc[w]+merged_data[\"text_neg\"].iloc[w]!=0):#分母不可為0\n",
    "            merged_data.iloc[w, 15] = merged_data[\"text_pos\"].iloc[w]\\\n",
    "            /(merged_data[\"text_pos\"].iloc[w]+merged_data[\"text_neg\"].iloc[w])#text_Pos_ratio\n",
    "\n",
    "            merged_data.iloc[w, 16] = merged_data[\"text_neg\"].iloc[w]\\\n",
    "            /(merged_data[\"text_pos\"].iloc[w]+merged_data[\"text_neg\"].iloc[w])#text_Neg_ratio\n",
    "\n",
    "            merged_data.iloc[w, 18] = (merged_data[\"text_pos\"].iloc[w]-merged_data[\"text_neg\"].iloc[w])\\\n",
    "            /(merged_data[\"text_pos\"].iloc[w]+merged_data[\"text_neg\"].iloc[w])#text_Polarity     \n",
    "        else:\n",
    "            merged_data.iloc[w, 15] = 0 #text_Pos_ratio\n",
    "            merged_data.iloc[w, 16] = 0 #text_Neg_ratio\n",
    "            merged_data.iloc[w, 18] = 0 #text_Polarity\n",
    "\n",
    "        if(merged_data[\"text_total\"].iloc[w] != 0):#分母不可為0\n",
    "            merged_data.iloc[w, 17] =  merged_data[\"text_netural\"].iloc[w]/merged_data[\"text_total\"].iloc[w]#text_Netural_ratio\n",
    "            merged_data.iloc[w, 19] = (merged_data[\"text_pos\"].iloc[w]+merged_data[\"text_neg\"].iloc[w])\\\n",
    "            /merged_data[\"text_total\"].iloc[w]#text_Subjectivity\n",
    "        else:\n",
    "            merged_data.iloc[w, 17] = 0 #text_Netural_ratio\n",
    "            merged_data.iloc[w, 19] = 0 #text_Subjectivity    \n",
    "            \n",
    "    csv = files[i].replace('_analyzed.csv', '_merged.csv')\n",
    "    output = os.path.join(os.getcwd(),'CNN_NoCategory2','CNN_NoCategory_merged',csv)\n",
    "    merged_data.to_csv(output, index=False)\n",
    "    print(i, csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.標準化情緒指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standsrd(x):\n",
    "    return (x - x.mean(skipna=True))/ x.std(skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'CNN_NoCategory2','CNN_NoCategory_merged')\n",
    "files = [i for i in os.listdir(path) if not i.startswith('.')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    \n",
    "    data_read = os.path.join(os.getcwd(),'CNN_NoCategory2','CNN_NoCategory_merged', files[i])\n",
    "    data = pd.read_csv(data_read)\n",
    "    \n",
    "    data[\"headline_sPos_ratio\"]     = None # 20\n",
    "    data[\"headline_sNeg_ratio\"]     = None # 21\n",
    "    data[\"headline_sNetural_ratio\"] = None # 22\n",
    "    data[\"headline_sPolarity\"]      = None # 23\n",
    "    data[\"headline_sSubjectivity\"]  = None # 24\n",
    "\n",
    "    data[\"text_sPos_ratio\"]         = None # 25\n",
    "    data[\"text_sNeg_ratio\"]         = None # 26\n",
    "    data[\"text_sNetural_ratio\"]     = None # 27\n",
    "    data[\"text_sPolarity\"]          = None # 28\n",
    "    data[\"text_sSubjectivity\"]      = None # 29\n",
    "\n",
    "    data['headline_sPos_ratio']     = standsrd(data[\"headline_Pos_ratio\"])\n",
    "    data['headline_sNeg_ratio']     = standsrd(data[\"headline_Neg_ratio\"])\n",
    "    data['headline_sNetural_ratio'] = standsrd(data[\"headline_Netural_ratio\"])\n",
    "    data['headline_sPolarity']      = standsrd(data[\"headline_Polarity\"])\n",
    "    data['headline_sSubjectivity']  = standsrd(data[\"headline_Subjectivity\"])\n",
    "\n",
    "\n",
    "    data['text_sPos_ratio']     = standsrd(data[\"text_Pos_ratio\"])\n",
    "    data['text_sNeg_ratio']     = standsrd(data[\"text_Neg_ratio\"])\n",
    "    data['text_sNetural_ratio'] = standsrd(data[\"text_Netural_ratio\"])\n",
    "    data['text_sPolarity']      = standsrd(data[\"text_Polarity\"])\n",
    "    data['text_sSubjectivity']  = standsrd(data[\"text_Subjectivity\"])\n",
    "    \n",
    "    csv = files[i].replace('_merged.csv', '.csv')\n",
    "    out_path = os.path.join(os.getcwd(),'CNN_NoCategory2','CNN_NoCategory', csv)\n",
    "    data.to_csv( out_path, index = False )\n",
    "    print('***',i,csv,'***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.處理Control variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用CNN有的company_symbol 作為 symbol\n",
    "read = os.path.join(os.getcwd(), 'CNN_NoCategory2', 'CNN_NoCategory')\n",
    "CNN_files = [i for i in os.listdir(read) if not i.startswith('.')]\n",
    "len(CNN_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily path\n",
    "daily_path = os.path.join(os.getcwd(),'daily_0628')\n",
    "daily_files = [i for i in os.listdir(daily_path) if not i.startswith('.')]\n",
    "len(daily_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market_cap path\n",
    "cap_path = os.path.join(os.getcwd(),'S&P500_marketcap_0622')\n",
    "cap_files = [i for i in os.listdir(cap_path) if not i.startswith('.')]\n",
    "len(cap_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CNN_files)):\n",
    "    symbol = CNN_files[i]\n",
    "    print(i, CNN_files[i])\n",
    "    \n",
    "    ### daily處理\n",
    "    daily_read = os.path.join(daily_path, symbol) # 讀取對應公司的daily資料\n",
    "    daily_data = pd.read_csv(daily_read)\n",
    "\n",
    "    daily_data['return']     = None # 6\n",
    "    daily_data['adj_return'] = None\n",
    "    daily_data['volume']     = None\n",
    "    daily_data['volatility'] = None\n",
    "    daily_data['spread']     = None\n",
    "\n",
    "    # 計算 return & adj_close\n",
    "    daily_data['return'] = daily_data['Close'] / daily_data['Close'].shift(1)\n",
    "    daily_data['return'] = daily_data['return'].apply(lambda x: math.log(x))\n",
    "    \n",
    "    daily_data['adj_return'] = daily_data['Adj Close'] / daily_data['Adj Close'].shift(1)\n",
    "    daily_data['adj_return'] = daily_data['adj_return'].apply(lambda x: math.log(x))\n",
    "    \n",
    "    # 計算 volatility & spread \n",
    "    High  = daily_data['High'].apply(lambda x: math.log(x))\n",
    "    Open  = daily_data['Open'].apply(lambda x: math.log(x))\n",
    "    Close = daily_data['Close'].apply(lambda x: math.log(x))\n",
    "    Low   = daily_data['Low'].apply(lambda x: math.log(x))\n",
    "\n",
    "    daily_data['volatility'] = (High - Close)*(High - Open) + (Low - Close) * (Low - Open)\n",
    "    daily_data['spread'] = 4*( Close - ( Low+High )/2 )*( Close - ( Low.shift(-1)+High.shift(-1) ) /2)\n",
    "    daily_data['spread'] = daily_data['spread'].apply(lambda x: max(x,0)**(1/2))\n",
    "\n",
    "    ### capitial 處理\n",
    "    cap_read = os.path.join(cap_path, CNN_files[i]) # 讀取對應公司的市值資料\n",
    "    cap_data = pd.read_csv(cap_read)\n",
    "    # 將cap取log e\n",
    "    cap_data['Market_Cap'] = cap_data['Market_Cap'].apply(lambda x: math.log((x/1000000)))\n",
    "\n",
    "    ### merge data：根據market_cap的日期做merge，再移除沒有return的column\n",
    "    feature_data = pd.merge(daily_data, cap_data, how=\"right\", on=\"Date\")\n",
    "    feature_data['volume'] = (feature_data['Volume'] - feature_data['Volume'].mean())\\\n",
    "                            / feature_data['Volume'].mean() # 根據有cap的日期計算volume的mean\n",
    "    feature_data = feature_data.dropna() # 沒有做reindex，因為最後不會存入index\n",
    "#     feature_data = feature_data.dropna(subset=['return'])\n",
    "    \n",
    "    feature_data = feature_data.rename(columns = {'Market_Cap':'size'})\n",
    "\n",
    "    ### save\n",
    "    out_path = os.path.join(os.getcwd(), 'daily_CNN_feature3', CNN_files[i])\n",
    "    feature_data.to_csv(out_path, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.合併新聞＆股價資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily path\n",
    "daily_path = os.path.join(os.getcwd(),'daily_CNN_feature3')\n",
    "daily_files = [i for i in os.listdir(daily_path) if not i.startswith('.')]\n",
    "len(daily_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news path\n",
    "news_path = os.path.join(os.getcwd(),'CNN_NoCategory2','CNN_NoCategory')\n",
    "news_files = [i for i in os.listdir(daily_path) if not i.startswith('.')]\n",
    "len(news_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "news_feature = ['headline_sPos_ratio','headline_sNeg_ratio','headline_sNetural_ratio','headline_sPolarity',\\\n",
    "                'headline_sSubjectivity','text_sPos_ratio','text_sNeg_ratio','text_sNetural_ratio',\\\n",
    "                'text_sPolarity','text_sSubjectivity'] # 再merge之後需要fillna的值，先存成list\n",
    "use_num = 0\n",
    "for i in range(len(daily_files)):\n",
    "# for i in range(2):\n",
    "    print(daily_files[i])\n",
    "    ### daily處理\n",
    "    daily_read = os.path.join(daily_path, daily_files[i])\n",
    "    daily_data = pd.read_csv(daily_read)\n",
    "    daily_data = daily_data.drop(daily_data.loc[:, 'Open':'Volume'].columns, axis =1)\n",
    "    \n",
    "    ### news 處理\n",
    "    news_read = os.path.join(news_path, daily_files[i])\n",
    "    news_data = pd.read_csv(news_read)\n",
    "    news_data = news_data.rename(columns = {'date':'Date'}) # 便於合併到daily_feature \n",
    "    news_data = news_data.drop(news_data.loc[:, 'headline_pos':'text_Subjectivity'].columns, axis =1) #砍掉不必要的columns\n",
    "        \n",
    "    \n",
    "    ### merge daily & news：先不將news做t-1的動作，直接與return的日期做對應，接著直接shift\n",
    "    feature_data = pd.merge(daily_data, news_data, how=\"left\", on=\"Date\")\n",
    "    use_num += feature_data['text_sPos_ratio'].notna().sum() # 計算最後所用到的新聞數量\n",
    "    feature_data[news_feature] = feature_data[news_feature].fillna(value=0) #將news_feature中nan值補0\n",
    "    \n",
    "    shift_list = feature_data.loc[:, 'return':'text_sSubjectivity'].columns.tolist() # 需要sfift的columns做成list\n",
    "    for j in range(1,6):\n",
    "        for name in shift_list:\n",
    "            feature_data[name+ '_lag' + str(j)] = feature_data[ name ].shift(j)\n",
    "            \n",
    "    feature_data = feature_data.drop(feature_data.loc[:, 'volume':'text_sSubjectivity'].columns, axis =1) #刪掉不必要的欄位\n",
    "    feature_data = feature_data.iloc[5:] # 砍掉前五筆經過shift後有nan值得row\n",
    "    feature_data.reset_index(inplace = True)\n",
    "    feature_data = feature_data.drop(columns = [\"index\"] )\n",
    "    \n",
    "    ### 每家公司處理完先獨立存在folder\n",
    "    feature_data.insert(0, 'company', '') # 填入公司代號\n",
    "    feature_data['company'] = daily_files[i].replace('.csv','')\n",
    "    feature_data.to_csv(os.path.join(os.getcwd(), 'Feature_CNN_Nocate3', daily_files[i] ), index= False)\n",
    "    \n",
    "    ### 將每家公司concat\n",
    "    if len(df) != 0:\n",
    "        df = pd.concat([df, feature_data])\n",
    "    else:\n",
    "        df = pd.concat([feature_data])\n",
    "df.to_csv(os.path.join(os.getcwd(), 'CNN_Nocate_Regression3.csv'), index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from linearmodels.panel import RandomEffects\n",
    "from linearmodels.panel import PanelOLS\n",
    "from linearmodels.panel import compare\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv( os.path.join(os.getcwd(), 'CNN_Nocate_Regression3.csv' ) )\n",
    "data['Date'] = data['Date'].apply( lambda x: datetime.strptime(x, '%Y-%m-%d').date() )\n",
    "data = data.set_index([\"company\", \"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = ['news_variable',\\\n",
    "        'return_lag1','volume_lag1','volatility_lag1','spread_lag1','size_lag1',\\\n",
    "        'return_lag2','volume_lag2','volatility_lag2','spread_lag2','size_lag2',\\\n",
    "        'return_lag3','volume_lag3','volatility_lag3','spread_lag3','size_lag3',\\\n",
    "        'return_lag4','volume_lag4','volatility_lag4','spread_lag4','size_lag4',\\\n",
    "        'return_lag5','volume_lag5','volatility_lag5','spread_lag5','size_lag5']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = ['headline_sPos_ratio_lag', 'headline_sNeg_ratio_lag', 'headline_sNetural_ratio_lag', 'headline_sPolarity_lag', 'headline_sSubjectivity_lag']\n",
    "text = ['text_sPos_ratio_lag', 'text_sNeg_ratio_lag', 'text_sNetural_ratio_lag', 'text_sPolarity_lag', 'text_sSubjectivity_lag']\n",
    "news_var = [head, text]\n",
    "for name in news_var:\n",
    "    for i in range(len(name)):\n",
    "        x = name[i]+str(1)\n",
    "        x_var[0] = x\n",
    "        print('*** news_var:',x ,'len:',len(x_var))\n",
    "        \n",
    "        y = data['adj_return']\n",
    "        exog_vars = x_var\n",
    "        exog = sm.add_constant(data[exog_vars])\n",
    "        mod = PanelOLS(y, exog,  time_effects = True, entity_effects = True )\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # pos = x\n",
    "            pos = mod.fit()\n",
    "        elif i % 5 == 1:\n",
    "            # neg = x\n",
    "            neg = mod.fit()\n",
    "        elif i % 5 == 2:\n",
    "            # netural = x\n",
    "            netural = mod.fit()\n",
    "        elif i % 5 == 3:\n",
    "            # polarity = x\n",
    "            polarity = mod.fit()\n",
    "        else:\n",
    "            # subjectivity = x\n",
    "            subjectivity = mod.fit()\n",
    "\n",
    "    # print(pos, neg, netural, polarity, subjectivity)\n",
    "    output = compare({\"Pos\": pos, \"Neg\": neg, \"Netural\":netural, \"Polarity\": polarity, \"Subjectivity\": subjectivity},precision= 'tstats', stars=True)\n",
    "    result = output.summary.as_csv()\n",
    "    result = result.replace(\"Model Comparison\",\"\")\n",
    "    result = \",\".join(result.split(\"=\")[:-1])\n",
    "    result = result.replace(\" \", \"\")\n",
    "    result = [j for j in result.split(\",\") if j != \"\"]\n",
    "    result = \",\".join(result)\n",
    "    result = result.replace(\"\\n,\\n\", \"\\n\")\n",
    "\n",
    "    with open (os.path.join(os.getcwd(),'cnn_singlePanelOLS_adj.csv'), 'a') as file:\n",
    "        file.write(result)\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_var = ['headline_sPos_ratio_lag', 'headline_sNeg_ratio_lag', 'headline_sNetural_ratio_lag', 'headline_sPolarity_lag', 'headline_sSubjectivity_lag',\\\n",
    "            'text_sPos_ratio_lag', 'text_sNeg_ratio_lag', 'text_sNetural_ratio_lag', 'text_sPolarity_lag', 'text_sSubjectivity_lag']\n",
    "\n",
    "multi_var = ['one', 'two', 'three', 'four', 'five',\\\n",
    "        'adj_return_lag1','volume_lag1','volatility_lag1','spread_lag1','size_lag1',\\\n",
    "        'adj_return_lag2','volume_lag2','volatility_lag2','spread_lag2','size_lag2',\\\n",
    "        'adj_return_lag3','volume_lag3','volatility_lag3','spread_lag3','size_lag3',\\\n",
    "        'adj_return_lag4','volume_lag4','volatility_lag4','spread_lag4','size_lag4',\\\n",
    "        'adj_return_lag5','volume_lag5','volatility_lag5','spread_lag5','size_lag5']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### one panel: multipule news variable\n",
    "for var in news_var:\n",
    "    # print(var)\n",
    "    for i in range(1,6):\n",
    "        x = var + str(i)\n",
    "        multi_var[i-1] = x\n",
    "    print('*** muti_var:',x ,'len:',len(multi_var))\n",
    "    \n",
    "    y = data['adj_return']\n",
    "    exog_vars = multi_var\n",
    "    exog = sm.add_constant(data[exog_vars])\n",
    "\n",
    "    ### PanelOLS: time_effects\n",
    "    mod = PanelOLS(y, exog, time_effects = True, entity_effects = True)\n",
    "    fixedPanel = mod.fit()\n",
    "    \n",
    "    ### compare all panel\n",
    "    output = compare({\"PanelOLS\": fixedPanel}, precision='tstats', stars=True)\n",
    "    # print(output)\n",
    "    result = output.summary.as_csv()\n",
    "    result = result.replace(\"Model Comparison\",\"\")\n",
    "    result = \",\".join(result.split(\"=\")[:-1])\n",
    "    result = result.replace(\" \", \"\")\n",
    "    result = [j for j in result.split(\",\") if j != \"\"]\n",
    "    result = \",\".join(result)\n",
    "    result = result.replace(\"\\n,\\n\", \"\\n\")\n",
    "\n",
    "    with open (os.path.join(os.getcwd(),'cnn_multiPanelOLS_adj.csv'), 'a') as file:\n",
    "        file.write(result)\n",
    "\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
